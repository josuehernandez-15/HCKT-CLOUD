service: alerta-utec-analitica

provider:
  name: aws
  runtime: python3.10
  region: us-east-1
  memorySize: 512
  timeout: 60
  iam:
    role: arn:aws:iam::${env:AWS_ACCOUNT_ID}:role/LabRole
  environment:
    TABLE_INCIDENTES: ${env:TABLE_INCIDENTES}
    TABLE_LOGS: ${env:TABLE_LOGS}
    TABLE_EMPLEADOS: ${env:TABLE_EMPLEADOS}
    ANALITICA_S3_BUCKET: ${env:ANALITICA_S3_BUCKET}
    ANALITICA_GLUE_DATABASE: ${env:ANALITICA_GLUE_DATABASE}
  layers:
    - ${cf:alerta-utec-dependencias-dev.PythonDependenciesLayerExport}

functions:
  # FunciÃ³n para subir el DAG a S3
  UploadDAG:
    handler: handler.upload_dag
    timeout: 30
    environment:
      ANALITICA_S3_BUCKET: ${env:ANALITICA_S3_BUCKET}
    events:
      - schedule:
          rate: rate(1 day)
          enabled: false

  # Lambda 1: AnÃ¡lisis de incidentes por piso y estado
  AnalisisIncidentesPorPiso:
    handler: handler.analisis_incidentes_por_piso
    timeout: 29
    events:
      - http:
          path: analitica/incidentes-por-piso
          method: get
          cors: true

  # Lambda 2: AnÃ¡lisis de incidentes por tipo y urgencia
  AnalisisIncidentesPorTipo:
    handler: handler.analisis_incidentes_por_tipo
    timeout: 29  # âœ… Ajustado a 29s
    events:
      - http:
          path: analitica/incidentes-por-tipo
          method: get
          cors: true

  # Lambda 3: AnÃ¡lisis de tiempo de resoluciÃ³n
  AnalisisTiempoResolucion:
    handler: handler.analisis_tiempo_resolucion
    timeout: 29 
    events:
      - http:
          path: analitica/tiempo-resolucion
          method: get
          cors: true

  # Lambda 4: AnÃ¡lisis de reportes por usuario
  AnalisisReportesPorUsuario:
    handler: handler.analisis_reportes_por_usuario
    timeout: 29 
    events:
      - http:
          path: analitica/reportes-por-usuario
          method: get
          cors: true

  # Lambda 5: Trigger ETL Pipeline
  TriggerETL:
    handler: handler.trigger_etl_pipeline
    timeout: 29
    events:
      - http:
          path: analitica/trigger-etl
          method: post
          cors: true

resources:
  Resources:
    # Log Group para Airflow
    AnaliticaAirflowLogGroup:
      Type: AWS::Logs::LogGroup
      Properties:
        LogGroupName: /ecs/alerta-utec-airflow
        RetentionInDays: 14

    # Cluster ECS
    AnaliticaAirflowCluster:
      Type: AWS::ECS::Cluster
      Properties:
        ClusterName: alerta-utec-analitica-cluster

    # Security Group para Airflow
    AnaliticaAirflowSecurityGroup:
      Type: AWS::EC2::SecurityGroup
      Properties:
        GroupDescription: Access control for Airflow on ECS
        VpcId: ${env:ANALITICA_VPC_ID}
        SecurityGroupIngress:
          # âœ… Permitir acceso desde cualquier IP (Lambda fuera de VPC)
          - IpProtocol: tcp
            FromPort: 8080
            ToPort: 8080
            CidrIp: 0.0.0.0/0
            Description: "Access from Lambda and external"
        SecurityGroupEgress:
          - IpProtocol: "-1"
            CidrIp: 0.0.0.0/0

    # Task Definition
    AnaliticaAirflowTaskDefinition:
      Type: AWS::ECS::TaskDefinition
      Properties:
        Family: alerta-utec-analitica-airflow
        Cpu: "2048"
        Memory: "4096"
        NetworkMode: awsvpc
        RequiresCompatibilities:
          - FARGATE
        ExecutionRoleArn: !Sub arn:aws:iam::${AWS::AccountId}:role/LabRole
        TaskRoleArn: !Sub arn:aws:iam::${AWS::AccountId}:role/LabRole
        ContainerDefinitions:
          - Name: airflow
            Image: apache/airflow:2.9.2-python3.10
            Essential: true
            PortMappings:
              - ContainerPort: 8080
                Protocol: tcp
            LogConfiguration:
              LogDriver: awslogs
              Options:
                awslogs-group: /ecs/alerta-utec-airflow
                awslogs-region: ${env:AWS_REGION, 'us-east-1'}
                awslogs-stream-prefix: airflow
            Environment:
              - Name: AWS_DEFAULT_REGION
                Value: ${env:AWS_REGION, 'us-east-1'}
              - Name: AWS_REGION
                Value: ${env:AWS_REGION, 'us-east-1'}
              - Name: AWS_ACCOUNT_ID
                Value: !Sub ${AWS::AccountId}
              - Name: ANALITICA_TABLES
                Value: ${env:ANALITICA_TABLES}
              - Name: ANALITICA_S3_BUCKET
                Value: ${env:ANALITICA_S3_BUCKET}
              - Name: ANALITICA_GLUE_DATABASE
                Value: ${env:ANALITICA_GLUE_DATABASE}
              - Name: ANALITICA_GLUE_CRAWLER
                Value: ${env:ANALITICA_GLUE_CRAWLER}
              - Name: AIRFLOW__CORE__EXECUTOR
                Value: SequentialExecutor
              - Name: AIRFLOW__CORE__LOAD_EXAMPLES
                Value: "False"
              - Name: AIRFLOW__CORE__DAGS_FOLDER
                Value: /opt/airflow/dags
              - Name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
                Value: sqlite:////opt/airflow/airflow.db
              - Name: AIRFLOW__WEBSERVER__EXPOSE_CONFIG
                Value: "True"
              - Name: AIRFLOW__API__AUTH_BACKENDS
                Value: "airflow.api.auth.backend.basic_auth"
              - Name: AIRFLOW__API__ENABLE_EXPERIMENTAL_API
                Value: "True"
              - Name: _AIRFLOW_DB_MIGRATE
                Value: "true"
              - Name: _AIRFLOW_WWW_USER_CREATE
                Value: "true"
              - Name: _AIRFLOW_WWW_USER_USERNAME
                Value: admin
              - Name: _AIRFLOW_WWW_USER_PASSWORD
                Value: admin
            EntryPoint:
              - /bin/bash
              - -c
            Command:
              - |
                set -e
                echo "=========================================="
                echo "ðŸš€ Iniciando Airflow en ECS (SequentialExecutor)"
                echo "=========================================="
                
                echo "ðŸ“¦ Instalando dependencias..."
                pip install --quiet --no-cache-dir boto3 awscli 2>&1 | grep -v "Requirement already satisfied" || true
                
                echo "ðŸ“ Creando directorio de DAGs..."
                mkdir -p /opt/airflow/dags
                
                echo "ðŸ“¥ Intentando descargar DAG desde S3..."
                MAX_RETRIES=5
                RETRY_COUNT=0
                DAG_DOWNLOADED=false
                
                while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
                  if aws s3 ls "s3://${ANALITICA_S3_BUCKET}/dags/etl_dynamodb.py" 2>/dev/null; then
                    echo "âœ“ DAG encontrado en S3, descargando..."
                    if aws s3 cp "s3://${ANALITICA_S3_BUCKET}/dags/etl_dynamodb.py" /opt/airflow/dags/etl_dynamodb.py; then
                      echo "âœ“ DAG descargado exitosamente"
                      DAG_DOWNLOADED=true
                      break
                    fi
                  fi
                  
                  RETRY_COUNT=$((RETRY_COUNT + 1))
                  if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                    echo "â³ Intento $RETRY_COUNT/$MAX_RETRIES - Esperando 10 segundos..."
                    sleep 10
                  fi
                done
                
                if [ "$DAG_DOWNLOADED" = false ]; then
                  echo "âš ï¸  No se pudo descargar el DAG despuÃ©s de $MAX_RETRIES intentos"
                  echo "ðŸ“ Creando DAG de ejemplo..."
                  cat > /opt/airflow/dags/ejemplo_dag.py << 'EOFDAG'
                from datetime import datetime
                from airflow import DAG
                from airflow.operators.python import PythonOperator
                
                def hello_world():
                    print("=" * 60)
                    print("ðŸŽ‰ Airflow estÃ¡ funcionando en ECS!")
                    print("=" * 60)
                
                with DAG(
                    'ejemplo_dag',
                    description='DAG de ejemplo',
                    start_date=datetime(2024, 1, 1),
                    schedule_interval='@daily',
                    catchup=False,
                    tags=['ejemplo']
                ) as dag:
                    task = PythonOperator(
                        task_id='hello_airflow',
                        python_callable=hello_world
                    )
                EOFDAG
                fi
                
                echo "ðŸ—„ï¸  Inicializando base de datos de Airflow..."
                airflow db migrate
                
                echo "ðŸ‘¤ Creando usuario admin..."
                airflow users create \
                  --username admin \
                  --password admin \
                  --firstname Admin \
                  --lastname User \
                  --role Admin \
                  --email admin@example.com 2>&1 | grep -v "already exists" || true
                
                echo "=========================================="
                echo "âœ… Iniciando Airflow Standalone"
                echo "ðŸ“Š WebUI: http://localhost:8080"
                echo "ðŸ‘¤ Usuario: admin / admin"
                echo "ðŸ”‘ API REST habilitada en /api/v1/"
                echo "=========================================="
                
                exec airflow standalone

    # ECS Service
    AnaliticaAirflowService:
      Type: AWS::ECS::Service
      DependsOn:
        - AnaliticaAirflowTaskDefinition
      Properties:
        ServiceName: alerta-utec-analitica-airflow
        Cluster: !Ref AnaliticaAirflowCluster
        LaunchType: FARGATE
        DesiredCount: 1
        TaskDefinition: !Ref AnaliticaAirflowTaskDefinition
        NetworkConfiguration:
          AwsvpcConfiguration:
            AssignPublicIp: ENABLED
            Subnets:
              Fn::Split:
                - ","
                - ${env:ANALITICA_SUBNETS}
            SecurityGroups:
              - !Ref AnaliticaAirflowSecurityGroup

  Outputs:
    AnaliticaAirflowServiceName:
      Value: !Ref AnaliticaAirflowService
      Export:
        Name: alerta-utec-analitica-airflow-service
    
    AnaliticaAirflowCluster:
      Value: !Ref AnaliticaAirflowCluster
      Export:
        Name: alerta-utec-analitica-cluster

